{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building an Operational Data Store with Firestore, Memorystore, and GCS\n",
                "\n",
                "This notebook implements a multi-layer ODS approach using Google Cloud services. \n",
                "\n",
                "**Architecture (Google Native):**\n",
                "1. **Operational Layer (Firestore):** Stores recent orders and serves individual document lookups.\n",
                "2. **Aggregate Document Pattern (Firestore):** Provides pre-computed metrics for standard dashboards. Efficient and cost-effective.\n",
                "3. **Speed Layer (Memorystore for Redis - Optional):** For extreme throughput and sub-millisecond latency. **Memorystore** is Google's managed version of Redis.\n",
                "4. **Archive Layer (GCS):** Stores monthly exports for long-term durability and compliance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 0: Setup\n",
                "\n",
                "Install necessary packages if not already present:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install google-cloud-firestore google-cloud-storage redis python-dotenv pandas numpy --quiet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "from datetime import datetime, timedelta\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from google.cloud import firestore, storage\n",
                "import redis\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "print(\"Imports OK\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Credentials\n",
                "\n",
                "Load credentials from the `.env` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "load_dotenv(Path(\"../.env\"))\n",
                "\n",
                "GCP_PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
                "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
                "\n",
                "REDIS_HOST = os.getenv(\"REDIS_HOST\")\n",
                "REDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\n",
                "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n",
                "\n",
                "print(f\"Project ID: {GCP_PROJECT_ID}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Connect to Firestore and Memorystore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Connect to Firestore\n",
                "db = firestore.Client(project=GCP_PROJECT_ID)\n",
                "print(\"Connected to Firestore.\")\n",
                "\n",
                "# Connect to Memorystore (Redis)\n",
                "r = None\n",
                "if REDIS_HOST and REDIS_HOST != \"YOUR_REDIS_HOST\":\n",
                "    try:\n",
                "        r = redis.Redis(\n",
                "            host=REDIS_HOST,\n",
                "            port=REDIS_PORT,\n",
                "            password=REDIS_PASSWORD,\n",
                "            decode_responses=True,\n",
                "        )\n",
                "        r.ping()\n",
                "        print(\"Connected to Memorystore (Redis).\")\n",
                "    except Exception as e:\n",
                "        print(f\"Memorystore connection failed: {e}. Caching section will be skipped.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Load Data into Firestore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "\n",
                "CATEGORIES = [\"footwear\", \"apparel\", \"accessories\", \"electronics\"]\n",
                "products_data = []\n",
                "for i in range(1, 21):\n",
                "    category = np.random.choice(CATEGORIES)\n",
                "    products_data.append({\n",
                "        \"product_id\": f\"PROD_{i:03d}\",\n",
                "        \"name\": f\"{category.title()} Item {i}\",\n",
                "        \"category\": category,\n",
                "        \"price\": round(np.random.uniform(20, 300), 2),\n",
                "    })\n",
                "\n",
                "REGIONS = [\"North\", \"South\", \"East\", \"West\"]\n",
                "STATUSES = [\"pending\", \"shipped\", \"delivered\", \"returned\"]\n",
                "orders_data = []\n",
                "start_date = datetime.now() - timedelta(days=35)\n",
                "\n",
                "for i in range(1, 101):\n",
                "    product = np.random.choice(products_data)\n",
                "    quantity = np.random.randint(1, 4)\n",
                "    order_date = start_date + timedelta(days=int(np.random.randint(0, 35)),\n",
                "                                        hours=int(np.random.randint(0, 24)))\n",
                "    orders_data.append({\n",
                "        \"order_id\": f\"ORD_{i:05d}\",\n",
                "        \"customer_id\": f\"CUST_{np.random.randint(1, 101):04d}\",\n",
                "        \"region\": np.random.choice(REGIONS),\n",
                "        \"product_id\": product[\"product_id\"],\n",
                "        \"category\": product[\"category\"],\n",
                "        \"unit_price\": product[\"price\"],\n",
                "        \"quantity\": quantity,\n",
                "        \"total\": round(product[\"price\"] * quantity, 2),\n",
                "        \"status\": np.random.choice(STATUSES, p=[0.05, 0.15, 0.75, 0.05]),\n",
                "        \"created_at\": order_date,\n",
                "    })\n",
                "\n",
                "# Batch Insert\n",
                "batch = db.batch()\n",
                "orders_ref = db.collection(\"orders\")\n",
                "for order in orders_data:\n",
                "    doc_ref = orders_ref.document(order[\"order_id\"])\n",
                "    batch.set(doc_ref, order)\n",
                "batch.commit()\n",
                "print(f\"Inserted {len(orders_data)} orders into Firestore.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: The 'Aggregate Document' Pattern\n",
                "\n",
                "Maintain a pre-computed summary document in Firestore."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def update_aggregate():\n",
                "    cutoff = datetime.now() - timedelta(days=30)\n",
                "    docs = db.collection(\"orders\").where(\"created_at\", \">=\", cutoff).where(\"status\", \"!=\", \"returned\").stream()\n",
                "    \n",
                "    summary = {}\n",
                "    for doc in docs:\n",
                "        d = doc.to_dict()\n",
                "        cat = d[\"category\"]\n",
                "        if cat not in summary:\n",
                "            summary[cat] = {\"revenue\": 0, \"orders\": 0}\n",
                "        summary[cat][\"revenue\"] += d[\"total\"]\n",
                "        summary[cat][\"orders\"] += 1\n",
                "        \n",
                "    db.collection(\"aggregates\").document(\"category_revenue\").set({\n",
                "        \"last_updated\": firestore.SERVER_TIMESTAMP,\n",
                "        \"data\": summary\n",
                "    })\n",
                "    print(\"Aggregate document updated in Firestore.\")\n",
                "    return summary\n",
                "\n",
                "current_summary = update_aggregate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: The Speed Layer (Memorystore - Optional)\n",
                "\n",
                "For extreme performance, we can push the aggregate summary further out into **Memorystore (Redis)**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sync_to_memorystore(summary_data):\n",
                "    if r:\n",
                "        # Cache for 1 hour (extreme speed layer)\n",
                "        r.set(\"dashboard:summary\", json.dumps(summary_data), ex=3600)\n",
                "        print(\"Metrics synced to Google Cloud Memorystore.\")\n",
                "    else:\n",
                "        print(\"Memorystore not connected. Skipping speed layer sync.\")\n",
                "\n",
                "sync_to_memorystore(current_summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6: Efficient Dashboard Loading\n",
                "\n",
                "Try Memorystore first (0.5ms), fallback to Firestore Aggregate (10ms)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_dashboard_data():\n",
                "    # 1. Try Speed Layer (Memorystore)\n",
                "    if r:\n",
                "        cached = r.get(\"dashboard:summary\")\n",
                "        if cached:\n",
                "            print(\"[MEMORYSTORE HIT]\")\n",
                "            return json.loads(cached)\n",
                "    \n",
                "    # 2. Fallback to Operational Data Store (Firestore Aggregate)\n",
                "    print(\"[MEMORYSTORE MISS] Reading from Firestore Aggregate...\")\n",
                "    doc = db.collection(\"aggregates\").document(\"category_revenue\").get()\n",
                "    if doc.exists:\n",
                "        return doc.to_dict()[\"data\"]\n",
                "    \n",
                "    return {}\n",
                "\n",
                "data = get_dashboard_data()\n",
                "for cat, m in data.items():\n",
                "    print(f\"  {cat:<15}  ${m['revenue']:>8,.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 7: Archive to GCS\n",
                "\n",
                "Export month-to-date orders to JSONL and upload."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if GCS_BUCKET_NAME and GCS_BUCKET_NAME != \"YOUR_BUCKET_NAME\":\n",
                "    bucket = storage.Client(project=GCP_PROJECT_ID).bucket(GCS_BUCKET_NAME)\n",
                "    export_path = Path(\"./orders_archive.jsonl\")\n",
                "    \n",
                "    month_start = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
                "    docs = db.collection(\"orders\").where(\"created_at\", \">=\", month_start).stream()\n",
                "    \n",
                "    with open(export_path, \"w\") as f:\n",
                "        for doc in docs:\n",
                "            order = doc.to_dict()\n",
                "            order[\"created_at\"] = order[\"created_at\"].isoformat() \n",
                "            f.write(json.dumps(order) + \"\\n\")\n",
                "            \n",
                "    gcs_path = f\"shopstream/orders/{datetime.now().strftime('%Y/%m/%d')}/orders_archive.jsonl\"\n",
                "    bucket.blob(gcs_path).upload_from_filename(str(export_path))\n",
                "    print(f\"Archived to: gs://{GCS_BUCKET_NAME}/{gcs_path}\")\n",
                "else:\n",
                "    print(\"GCS_BUCKET_NAME not set. Archiving skipped.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}